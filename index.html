<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yi Jiang</title>
  
  <meta name="author" content="Yi Jiang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name><strong><font size="4px">Yi Jiang</font></strong></name>
              </p>
              <p>I am a Research Lead at Media Intelligence of ByteDance Ads core, where I work on computer vision and machine learning.
              </p>
              <p>
              I got my Master's degree from the Department of Computer Science and Engineering, Zhejiang University. 
              <p>
                My previous research focus is on instance understanding in spatial & temporal, including <b>large scale & open-vocabulary object detection, segmentation, tracking and Pretraining</b>. Most of my works have released codes on Github, with over <b>12.0K stars</b>.
              </p>
              <p>
                Recently, my research focuses on <b>visual foundation models</b>, <b>deep generative models</b>(Diffusion models and its application) and <b>large language models</b>.
              </p>
              <p style="text-align:center">
                <a href="mailto:jiangyi0425@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com.hk/citations?user=6dikuoYAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/iFighting/">Github</a>
              </p>
            </td>
            
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="./photos/jiangyi.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="./photos/jiangyi.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
<p style="text-align:left"><strong><font size="4px">Research Highlight</font></strong></p> 
		<ul>
      <li><p style="text-align:left"><b>UNINEXT</b> accepted by CVPR'23. UNINEXT unifies 10 instance perception tasks using a single model with the same model parameters.</p></li> 
		  
      <li><p style="text-align:left"><b>ByteTrack</b> ranks <a href="https://www.paperdigest.org/2023/01/most-influential-eccv-papers-2023-01/"; style="color: #EE7F2D;"> <b>1th of the most influential papers in ECCV 2022.</b></a> Code is available on github with 3.4k stars </p></li>
      
      <li><p style="text-align:left"><b>Spark</b> accepted by ICLR'23 as <b style="color: red">Spotlight. </b>Spark is the first successful BERT/MAE-style pretraining on any convolutional networks</p></li>
      
      <li><p style="text-align:left"><b>Unicorn</b> accepted by ECCV'22 as <b style="color: red">Oral Presentation</b> . Unicorn accomplishes the great unification of the tracking network architecture and learning paradigm</p></li>
      
      <li><p style="text-align:left"><b>IDOL</b> and <b>Seqformer</b> accepted by ECCV'22 as <b style="color: red">Oral Presentation</b> , serving as strong baseline for video instance segmentation. </p></li> 
      
      <li><p style="text-align:left"><b>Sparse R-CNN</b> accepted by CVPR'21. Sparse R-CNN is integrated into popular framework(Detectron2, MMDetection, PaddlePaddle)</p></li>
    </ul>
	 
	<br>


 
 <p style="text-align:justify"><strong><font size="4px">Publications</font></strong> (* equal contribution, <sup>â€ </sup> corresponding author)
		<table id="pubList" border="0" cellpadding="0" width="100%" style="border-spacing: 10 18px; line-height:16pt; border: 0px;">

	    <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/UNINEXT_CVPR2023.png" style="height: 120px; width: 300px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2303.06674.pdf">
              <papertitle><b>Universal Instance Perception as Object Discovery and Retrieval</b></papertitle>
              </a>
              <br>
	      Bin Yan, <b>Yi Jiang</b><sup>â€ </sup>, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, Huchuan Lu
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023
              <br>
	      <a href="https://arxiv.org/abs/2303.06674"; style="color: #EE7F2D;">Paper</a>
        /
              <a href="https://github.com/MasterBin-IIAU/UNINEXT"; style="color: #EE7F2D;">Code</a>
              <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/MasterBin-IIAU/UNINEXT.svg" alt="GitHub stars" title="" />
            </td><tr>
			
      <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/InstMove_CVPR2023.png" style="height: 120px; width: 300px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2303.08132.pdf">
              <papertitle><b>InstMove: Instance Motion for Object-centric Video Segmentation</b></papertitle>
              </a>
              <br>
        Qihao Liu, Junfeng Wu, <b>Yi Jiang</b>, Xiang Bai, Alan Yuille, Song Bai
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023
              <br>
        <a href="https://arxiv.org/abs/2303.08132"; style="color: #EE7F2D;">Paper</a>
        /
              <a href="https://github.com/wjf5203/vNext"; style="color: #EE7F2D;">Code</a>
              <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/wjf5203/vNext.svg" alt="GitHub stars" title="" />
            </td><tr>
	    
      <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/Spark_ICLR2023.png" style="height: 120px; width: 300px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/pdf?id=NRxydtWup1S">
              <papertitle><b>Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling</b></papertitle>
              </a>
              <br>
        Keyu Tian, <b>Yi Jiang</b><sup>â€ </sup>, Qishuai Diao, Chen Lin, Liwei Wang, Zehuan Yuan
              <br>
              <em>International Conference on Learning Representations (<b>ICLR</b>)</em>, 2023 <b style="color: red">[Spotlight notable-top-25% of Accepted Papers]</b> 
              <br>
        <a href="https://openreview.net/forum?id=NRxydtWup1S"; style="color: #EE7F2D;">Paper</a>
        /
              <a href="https://github.com/keyu-tian/SparK"; style="color: #EE7F2D;">Code</a>
              <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/keyu-tian/SparK.svg" alt="GitHub stars" title="" />
            </td><tr>

      
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/VLDet_ICLR2023.png" style="height: 120px; width: 300px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2211.14843.pdf">
              <papertitle><b>Learning Object-Language Alignments for Open-Vocabulary Object Detection,</b></papertitle>
              </a>
              <br>
        Chuang Lin, Peize Sun,  <b>Yi Jiang</b>, Ping Luo, Lizhen Qu, Gholamreza Haffari, Zehuan Yuan, Jianfei Cai
              <br>
              <em>International Conference on Learning Representations (<b>ICLR</b>)</em>, 2023
              <br>
        <a href="https://arxiv.org/abs/2211.14843"; style="color: #EE7F2D;">Paper</a>
        /
              <a href="https://github.com/clin1223/VLDet"; style="color: #EE7F2D;">Code</a>
              <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/clin1223/VLDet.svg" alt="GitHub stars" title="" />
            </td><tr>


        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/reskd_nips2022.png" style="height: 120px; width: 300px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/pdf?id=rnJzy8JnaX">
              <papertitle><b>Rethinking Resolution in the Context of Efficient Video Recognition</b></papertitle>
              </a>
              <br>
        Chuofan Ma, Qiushan Guo, <b>Yi Jiang</b><sup>â€ </sup>, Zehuan Yuan, Ping Luo, Xiaojuan Qi<sup>â€ </sup>
              <br>
              <em>Advances in Neural Information Processing Systems (<b>NeurIPS</b>)</em>, 2022
              <br>
        <a href="https://arxiv.org/abs/2209.12797"; style="color: #EE7F2D;">Paper</a>
        /
              <a href="https://github.com/CVMI-Lab/ResKD"; style="color: #EE7F2D;">Code</a>
              <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/CVMI-Lab/ResKD.svg" alt="GitHub stars" title="" />
            </td><tr>
	
		    

        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/Unicorn_ECCV2022.png" style="height: 120px; width: 300px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2207.07078.pdf">
              <papertitle><b>Unicorn ðŸ¦„ : Towards Grand Unification of Object Tracking</b></papertitle>
              </a>
              <br>
        Bin Yan, <b>Yi Jiang</b><sup>â€ </sup>, Peize Sun, Dong Wang, Zehuan Yuan, Ping Luo, Huchuan Lu
              <br>
              <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2022 <b style="color: red">[Oral Presentation Top 2.7%]</b> 
              <br>
        <a href="https://arxiv.org/abs/2207.07078"; style="color: #EE7F2D;">Paper</a>
        /
              <a href="https://github.com/MasterBin-IIAU/Unicorn"; style="color: #EE7F2D;">Code</a>
              <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/MasterBin-IIAU/Unicorn.svg" alt="GitHub stars" title="" />
            </td><tr>


        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/idol_ECCV2022.png" style="height: 120px; width: 300px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2207.10661.pdf">
              <papertitle><b>In Defense of Online Models for Video Instance Segmentation</b></papertitle>
              </a>
              <br>
        Junfeng Wu, QiHao Liu, <b>Yi Jiang</b>, Song Bai, Alan Yuille, Xiang Bai
              <br>
              <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2022 <b style="color: red">[Oral Presentation Top 2.7%]</b> 
              <br>
        <a href="https://arxiv.org/abs/2207.10661"; style="color: #EE7F2D;">Paper</a>
        /
              <a href="https://github.com/wjf5203/vNext"; style="color: #EE7F2D;">Code</a>
              <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/wjf5203/vNext.svg" alt="GitHub stars" title="" />
            </td><tr>

        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/seqformer_ECCV2022.png" style="height: 150px; width: 300px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2112.08275.pdf">
              <papertitle><b>SeqFormer: Sequential Transformer for Video Instance Segmentation</b></papertitle>
              </a>
              <br>
        Junfeng Wu, <b>Yi Jiang</b>, Song Bai, Wenqing Zhang, Xiang Bai
              <br>
              <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2022 <b style="color: red">[Oral Presentation Top 2.7%]</b> 
              <br>
        <a href="https://arxiv.org/abs/2112.08275"; style="color: #EE7F2D;">Paper</a>
        /
              <a href="https://github.com/wjf5203/vNext"; style="color: #EE7F2D;">Code</a>
              <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/wjf5203/vNext.svg" alt="GitHub stars" title="" />
            </td><tr>

        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/mtvm_ECCV2022.png" style="height: 120px; width: 300px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2111.05759.pdf">
              <papertitle><b>Multimodal Transformer with Variable-length Memory for Vision-and-Language Navigation</b></papertitle>
              </a>
              <br>
        Chuang Lin, <b>Yi Jiang</b><sup>â€ </sup>, Jianfei Cai, Lizhen Qu, Gholamreza Haffari, Zehuan Yuan
              <br>
              <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2022
              <br>
        <a href="https://arxiv.org/abs/2111.05759"; style="color: #EE7F2D;">Paper</a>
        /
              <a href="https://github.com/clin1223/MTVM"; style="color: #EE7F2D;">Code</a>
              <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/clin1223/MTVM.svg" alt="GitHub stars" title="" />
            </td><tr>

        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/bytetrack_ECCV2022.png" style="height: 150px; width: 240px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2110.06864.pdf">
              <papertitle><b>ByteTrack: Multi-Object Tracking by Associating Every Detection Box</b></papertitle>
              </a>
              <br>
        Yifu Zhang, Peize Sun, <b>Yi Jiang</b>, Dongdong Yu, Fucheng Weng, Zehuan Yuan, Ping Luo, Wenyu Liu, Xinggang Wang
              <br>
              <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2022
              <br>
        <a href="https://arxiv.org/abs/2111.05759"; style="color: #EE7F2D;">Paper</a>
        /
              <a href="https://github.com/ifzhang/ByteTrack"; style="color: #EE7F2D;">Code</a>
              <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/ifzhang/ByteTrack.svg" alt="GitHub stars" title="" />
            </td><tr>


        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/referformer_CVPR2022.png" style="height: 120px; width: 300px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2201.00487.pdf">
              <papertitle><b>Language as Queries for Referring Video Object Segmentation</b></papertitle>
              </a>
              <br>
        Jiannan Wu, <b>Yi Jiang</b>, Peize Sun, Zehuan Yuan, Ping Luo
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
              <br>
        <a href="https://arxiv.org/abs/2201.00487"; style="color: #EE7F2D;">Paper</a>
        /
              <a href="https://github.com/wjn922/ReferFormer"; style="color: #EE7F2D;">Code</a>
              <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/wjn922/ReferFormer.svg" alt="GitHub stars" title="" />
            </td><tr>

        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/dancetrack_CVPR2022.png" style="height: 120px; width: 300px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2111.14690.pdf">
              <papertitle><b>DanceTrack: Multi-Object Tracking in Uniform Appearance and Diverse Motion</b></papertitle>
              </a>
              <br>
        Peize Sun, Jinkun Cao, <b>Yi Jiang</b>, Zehuan Yuan, Song Bai, Kris Kitani, Ping Luo
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
              <br>
        <a href="https://arxiv.org/abs/2111.14690"; style="color: #EE7F2D;">Paper</a>
        /
              <a href="https://github.com/DanceTrack/DanceTrack"; style="color: #EE7F2D;">Code</a>
              <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/DanceTrack/DanceTrack.svg" alt="GitHub stars" title="" />
            </td><tr>


        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/topology_ICLR2022.png" style="height: 140px; width: 300px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2110.02687">
              <papertitle><b>Objects in Semantic Topology</b></papertitle>
              </a>
              <br>
        Shuo Yang, Peize Sun, <b>Yi Jiang</b>, Xiaobo Xia, Ruiheng Zhang, Zehuan Yuan, Changhu Wang, Ping Luo, Min Xu
              <br>
              <em>International Conference on Learning Representations (<b>ICLR</b>)</em>, 2022
              <br>
        <a href="https://arxiv.org/abs/2110.02687"; style="color: #EE7F2D;">Paper</a>
        </td><tr>

        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/onenet_ICML2021.png" style="height: 150px; width: 250px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2012.05780.pdf">
              <papertitle><b>What Makes for End-to-End Object Detection?</b></papertitle>
              </a>
              <br>
              Peize Sun, <b>Yi Jiang</b>, Enze Xie, Wenqi Shao, Zehuan Yuan, Changhu Wang, Ping Luo
              <br>
              <em>International Conference on Machine Learning (<b>ICML</b>)</em>, 2021
              <br>
        <a href="https://arxiv.org/abs/2012.05780"; style="color: #EE7F2D;">Paper</a>
        /
              <a href="https://github.com/PeizeSun/OneNet"; style="color: #EE7F2D;">Code</a>
              <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/PeizeSun/OneNet.svg" alt="GitHub stars" title="" />
            </td><tr>

          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/sparsercnn_CVPR2021.png" style="height: 100px; width: 320px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2011.12450.pdf">
              <papertitle><b>Sparse R-CNN: End-to-End Object Detection with Learnable Proposals</b></papertitle>
              </a>
              <br>
              Peize Sun<sup>*</sup>, Rufeng Zhang<sup>*</sup>, <b>Yi Jiang</b><sup>*</sup>, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, Ping Luo
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2021
              <br>
        <a href="https://arxiv.org/abs/2011.12450"; style="color: #EE7F2D;">Paper</a>
        /
              <a href="https://github.com/PeizeSun/SparseR-CNN"; style="color: #EE7F2D;">Code</a>
              <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/PeizeSun/SparseR-CNN.svg" alt="GitHub stars" title="" />
            </td><tr>



          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/densepose_TMM.png" style="height: 150px; width: 280px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9619945">
              <papertitle><b>Single person dense pose estimation via geometric equivariance consistency</b></papertitle>
              </a>
              <br>
              Qinchuan Zhang, <b>Yi Jiang</b>, Qin Zhou, Yiru Zhao, Yao Liu, Hongtao Lu, Xian-Sheng Hua
              <br>
              <em>IEEE Transactions on Multimedia (<b>TMM</b>)</em>, 2021
              <br>
        <a href="https://ieeexplore.ieee.org/document/9619945"; style="color: #EE7F2D;">Paper</a>
        </td><tr>


          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/lst_CVPR2020.png" style="height: 120px; width: 300px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2004.00900.pdf">
              <papertitle><b>Learning to Segment the Tail</b></papertitle>
              </a>
              <br>
              Xinting Hu, <b>Yi Jiang</b>, Kaihua Tang, Jingyuan Chen, Chunyan Miao, Hanwang Zhang
              <br>
              <em>Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2020
              <br>
        <a href="https://arxiv.org/abs/2004.00900"; style="color: #EE7F2D;">Paper</a>
        /
              <a href="https://github.com/JoyHuYY1412/LST_LVIS"; style="color: #EE7F2D;">Code</a>
              <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/JoyHuYY1412/LST_LVIS.svg" alt="GitHub stars" title="" />
            </td><tr>

	     
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/simpledet_JMLR.png" style="height: 120px; width: 300px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1903.05831.pdf">
              <papertitle><b>SimpleDet - A Simple and Versatile Framework for Object Detection and Instance Recognition</b></papertitle>
              </a>
              <br>
              Yuntao Chen, Chenxia Han, Yanghao Li, Zehao Huang, <b>Yi Jiang</b>, Naiyan Wang, Zhaoxiang Zhang
              <br>
              <em>Journal of Machine Learning Research (<b>JMLR</b>)</em>, 2019
              <br>
        <a href="https://arxiv.org/abs/1903.05831"; style="color: #EE7F2D;">Paper</a>
        /
              <a href="https://github.com/tusen-ai/simpledet"; style="color: #EE7F2D;">Code</a>
              <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/tusen-ai/simpledet.svg" alt="GitHub stars" title="" />
            </td><tr>

						       
</table></p>


<p style="text-align:justify"><strong><font size="4px">Preprints</font></strong> (* equal contribution, <sup>â€ </sup> corresponding author)
    <table id="pubList" border="0" cellpadding="0" width="100%" style="border-spacing: 10 18px; line-height:16pt; border: 0px;">
      
  
     <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/EGC_arxiv_2023.png" style="height: 120px; width: 300px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2304.02012.pdf">
              <papertitle><b>EGC: Image Generation and Classification via a Diffusion Energy-Based Model</b></papertitle>
              </a>
              <br>
              Qiushan Guo, Chuofan Ma, <b>Yi Jiang</b>, Zehuan Yuan, Yizhou Yu, and Ping Luo.              
              <br>
              <em>arXiv</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2304.02012"; style="color: #EE7F2D;">arXiv</a>
              /
              <a href="https://guoqiushan.github.io/egc.github.io/"; style="color: #EE7F2D;">Project</a>
              <p></p>
            </td><tr> 
      
     <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/MCL_1_arxiv_2023.png" style="height: 90px; width: 320px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2304.02010.pdf">
              <papertitle><b>Multi-Level Contrastive Learning for Dense Prediction Task</b></papertitle>
              </a>
              <br>
              Qiushan Guo, Yizhou Yu, <b>Yi Jiang</b>, Jiannan Wu, Zehuan Yuan, and Ping Luo.              
              <br>
              <em>arXiv</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2304.02010"; style="color: #EE7F2D;">arXiv</a>
              /
              <a href="https://github.com/GuoQiushan/MCL"; style="color: #EE7F2D;">Code</a>
              <p></p>
            </td><tr> 

     <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/motionMAE_arxiv_2022.png" style="height: 120px; width: 300px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2210.04154.pdf">
              <papertitle><b>Self-supervised Video Representation Learning with Motion-Aware Masked Autoencoders</b></papertitle>
              </a>
              <br>
              Haosen Yang, Deng Huang, Bin Wen, Jiannan Wu, Hongxun Yao, <b>Yi Jiang</b><sup>â€ </sup>, Xiatian Zhu, Zehuan Yuan              
              <br>
              <em>arXiv</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2210.04154"; style="color: #EE7F2D;">arXiv</a>
              /
              <a href="https://github.com/happy-hsy/MotionMAE"; style="color: #EE7F2D;">Code</a>
              <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/happy-hsy/MotionMAE" alt="GitHub stars" title="" />
              <p></p>
            </td><tr>


      <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/MetaFormer_arxiv_2022.png" style="height: 120px; width: 300px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2203.02751.pdf">
              <papertitle><b>MetaFormer: A Unified Meta Framework for Fine-Grained Recognition</b></papertitle>
              </a>
              <br>
              Qishuai Diao, <b>Yi Jiang</b><sup>â€ </sup>, Bin Wen, Jia Sun, Zehuan Yuan              
              <br>
              <em>arXiv</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2203.02751"; style="color: #EE7F2D;">arXiv</a>
              /
              <a href="https://github.com/dqshuai/MetaFormer"; style="color: #EE7F2D;">Code</a>
              <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/dqshuai/MetaFormer.svg" alt="GitHub stars" title="" />
              <p></p>
            </td><tr> 

      <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="./photos/TransTrack_arxiv_2021.png" style="height: 120px; width: 300px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2012.15460.pdf">
              <papertitle><b>TransTrack: Multiple Object Tracking with Transformer</b></papertitle>
              </a>
              <br>
              Peize Sun, Jinkun Cao, <b>Yi Jiang</b>, Rufeng Zhang, Enze Xie, Zehuan Yuan, Changhu Wang, Ping Luo              
              <br>
              <em>arXiv</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2012.15460"; style="color: #EE7F2D;">arXiv</a>
              /
              <a href="https://github.com/PeizeSun/TransTrack"; style="color: #EE7F2D;">Code</a>
              <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/PeizeSun/TransTrack.svg" alt="GitHub stars" title="" />
              <p></p>
            </td><tr> 
      
        </table></p>
		
	<p style="text-align:left"><strong><font size="4px">Awards</font></strong></p>
		<ul>
		    <li><p style="text-align:left">Winner of CVPR 2022 Large-scale Video Object Segmentation Challenge: Video Instance Segmentation</p></li>
        <li><p style="text-align:left">Runner up of CVPR 2021 FGVC8 iNaturalist Challenge</p></li>
        <li><p style="text-align:left">Runner up of ICCV 2019 WIDER Face and Person Challenge: Face Detection</p></li>
  			<li><p style="text-align:left">Excellent Mentor Award, ByteDance 2021</p></li>
        <li><p style="text-align:left">Outstanding Staff Award, Bytedance 2020</p></li>
        <li><p style="text-align:left">Competition Master in kaggle 2018</p></li>
		</ul><br>

	<p style="text-align:justify"><strong><font size="4px">Professional activities</font></strong></p>
		<ul>
      <li><p style="text-align:left">Main Organizer for <a href="https://motcomplex.github.io/"; style="color: #EE7F2D;"> <b>ECCV 2022 Workshop: Multiple Object Tracking and Segmentation in Complex Environments</b></a> </p></li>
			<li><p style="text-align:left">Reviewer for CVPR, ICCV, ECCV, NIPS, MM</p></li>
			<li><p style="text-align:left">Reviewer for Trans. on Pattern Analysis and Machine Intelligence (TPAMI), IEEE Trans. on Image Processing (TIP), Pattern Recognition</p></li>
	    </ur>

<script src="script.js"></script><table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
              Design and source code from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
              <br>
              </p>
            </td>
          </tr>
        </tbody></table>

</body>          

</html>
